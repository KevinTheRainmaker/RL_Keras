{"cells":[{"cell_type":"markdown","metadata":{"id":"1Q2mN5F7h-bS"},"source":["# 정책 이터레이션\n"]},{"cell_type":"markdown","metadata":{"id":"Ig1B1628iLyx"},"source":["해당 노트북은 정책 이터레이션을 그리드월드에서 구현하는 예제 소스코드 `policy_iteration.py`와 `environment.py`에 대한 정리 노트북이다.\n","\n","1. `policy_iteration.py`\n","\n","  Policyiteration 클래스를 포함하며, 해당 클래스에는 정책 이터레이션 함수 및 main 함수가 포함되어 있다.\n","\n","2. `environment.py`\n","\n","  그리드월드 예제 화면을 구성하고, 상태, 보상 등 환경에 대한 정보를 제공하기 위한 함수로 구성되어 있다. 해당 코드는 깃허브의 'rlcode/reinforcement-learning-kr-v2'로부터 가져왔다. \n","  \n","  그리드월드의 경우 환경을 직접 만든 것이기 때문에 이러한 파일이 필요하나, 일반적으로는 이미 구축되어 있는 환경에 강화학습을 적용하는 경우가 많으므로 에이전트와 관련된 파일만 필요한 경우가 많다."]},{"cell_type":"markdown","metadata":{"id":"RvdFQKUJjlk8"},"source":["## 1. `policy_iteration.py`"]},{"cell_type":"markdown","metadata":{"id":"us2_TGoiwFeC"},"source":["main 함수를 제외한 `policy_iteration.py`는 다음과 같다."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":521,"status":"ok","timestamp":1652685913460,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"q7fTJ_vcopD2"},"outputs":[],"source":["import numpy as np\n","from environment import GraphicDisplay, Env\n","\n","class Policyiteration:\n","  def __init__(self, env):\n","    self.env = env\n","\n","    # 가치함수를 2차원 리스트로 초기화\n","    self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","\n","    # 모두 동일한 확률로 초기화\n","    self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                         for _ in range(env.height)]\n","\n","    # terminal state 설정\n","    self.policy_table[2][2] = []\n","\n","    # 할인율 0.9\n","    self.discount_factor = 0.9\n","  \n","  # 벨만 기대 방정식으로 정책 평가\n","  def policy_evaluation(self):\n","    # 다음 가치함수 초기화\n","    next_value_table = [[0.00] * self.env.width\n","                        for _ in range(self.env.height)]\n","\n","    # 모든 상태에 대해서 벨만 기대 방정식 계산\n","    for state in self.env.get_all_states():\n","      value = 0.0\n","\n","      if state != [2,2]: \n","        continue\n","      else: # terminal state\n","        next_value_table[state[0]][state[1]] = value\n","\n","      # 벨만 기대 방정식\n","      for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state,action)\n","        next_value = self.get_value(next_state)\n","\n","        value += (self.get_policy(state)[action] *\n","                  (reward + self.discount_factor * next_value))\n","        \n","      next_value_table[state[0]][state[1]] = value\n","\n","    self.value_table = next_value_table\n","\n","  # 현재 가치함수에 대해 탐욕 정책 발전\n","  def policy_improvement(self):\n","        next_policy = self.policy_table\n","        for state in self.env.get_all_states():\n","            if state == [2, 2]:\n","                continue\n","            \n","            value_list = []\n","\n","            # 정책 초기화\n","            result = [0.0, 0.0, 0.0, 0.0]\n","\n","            # 모든 행동에 대해 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n","            for index, action in enumerate(self.env.possible_actions):\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                value = reward + self.discount_factor * next_value\n","                value_list.append(value)\n","\n","            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","            max_idx_list = max_idx_list.flatten().tolist()\n","            prob = 1 / len(max_idx_list)\n","\n","            for idx in max_idx_list:\n","                result[idx] = prob\n","\n","            next_policy[state[0]][state[1]] = result\n","\n","        self.policy_table = next_policy\n","\n","  # 특정 상태에서 정책에 따라 무작위로 행동 반환\n","  def get_action(self, state):\n","      policy = self.get_policy(state)\n","      policy = np.array(policy)\n","      return np.random.choice(4, 1, p=policy)[0]\n","\n","  # 상태에 따른 정책 반환\n","  def get_policy(self, state):\n","      return self.policy_table[state[0]][state[1]]\n","\n","  # 가치함수 값을 반환tar -xzvf tk.tar.gz\n","  def get_value(self, state):\n","      return self.value_table[state[0]][state[1]]"]},{"cell_type":"markdown","metadata":{"id":"ZRtySmjEjxQn"},"source":["`policy_iteration.py`에서는 먼저 `environment.py`의 GraphicDisplay와 Env 클래스를 import 한다."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1652683253858,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"xLfKV5r8hrkb"},"outputs":[],"source":["from environment import GraphicDisplay, Env"]},{"cell_type":"markdown","metadata":{"id":"NvmZt9zBn3qF"},"source":["정책 이터레이션의 에이전트는 `policy_iteration.py` 파일 내에서 Policyiteration 클래스로 정의되어 있다. Env 클래스로 정의한 객체 env를 Policyiteration 클래스의 인수로 전달함으로써 에이전트가 환경의 Env 클래스에 접근할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["다음 함수를 실행하면 그리드월드 화면이 나오게 된다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":290,"status":"error","timestamp":1652685853201,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"j-n7pWJamSQd","outputId":"845ba865-6add-4817-c266-f02d31084795"},"outputs":[],"source":["# main 함수\n","# if __name__ == '__main__':\n","#   env = Env()\n","#   policy_iteration = Policyiteration(env)\n","#   grid_world = GraphicDisplay(policy_iteration)\n","#   grid_world.mainloop()"]},{"cell_type":"markdown","metadata":{},"source":["**이제 `policy_iteration.py`의 코드를 하나씩 분석해보도록 하자.**"]},{"cell_type":"markdown","metadata":{},"source":["### \\_\\_init\\_\\_"]},{"cell_type":"markdown","metadata":{},"source":["\n","    def __init__(self, env):\n","        self.env = env\n","\n","        # 가치함수를 2차원 리스트로 초기화\n","        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","\n","        # 모두 동일한 확률로 초기화\n","        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                                for _ in range(env.height)]\n","\n","        # terminal state 설정\n","        self.policy_table[2][2] = []\n","\n","        # 할인율 0.9\n","        self.discount_factor = 0.9"]},{"cell_type":"markdown","metadata":{},"source":["먼저 Policyiteration 코드 내에서 가장 먼저 정의되는 \\_\\_init\\_\\_ 함수를 살펴보자.\n","\n","해당 함수는 먼저 env를 self.env로 정의함으로써 환경에 대한 객체를 선언한다.\n","\n","env 객체에 정의되어 있는 변수 및 함수는 다음과 같다."]},{"cell_type":"markdown","metadata":{},"source":["| 코드 | 설명 | 반환값|\n","|-----|-----|-----|\n","|`env.width`, `env.height`|그리드월드의 너비와 높이|그리드월드의 가로, 세로를 정수로 반환|\n","|`env.state_after_action(state, action)`|state에서 action을 했을 때 에이전트가 가는 다음 상태|행동 후의 상태 좌표를 리스트로 반환|\n","|`env.get_all_states()`|존재하는 모든 상태|모든 상태를 반환|\n","|`env.get_reward(state, action)`|state의 보상|보상을 정수 형태로 반환|\n","|`env.possible_actions`|상, 하, 좌, 우|[0,1,2,3] 반환. 순서대로 상하좌우|"]},{"cell_type":"markdown","metadata":{},"source":["정책 이터레이션은 모든 상태에 대해 가치함수를 계산하기 때문에 `value_table`이라는 2차원 리스트 변수로 가치함수를 선언한다.\n","\n","여기서 2차원 리스트의 크기는 `env.width`와 `env.height`로 정해지므로 그리드월드의 크기인 5*5이다.\n","\n","모든 상태의 가치함수에 대한 초기화 값은 0이다.\n","\n","\n","```\n","# 가치함수를 2차원 리스트로 초기화\n","self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["정책 `policy_table`은 모든 상태에 대해 상, 하, 좌, 우에 해당하는 각 행동의 확률을 담고 있는 리스트이다.\n","\n","따라서 5 * 5 * 4의 크기를 가지는 3차원 리스트이며, 모든 행동에 대하여 같은 확률인 0.25로 초기화를 진행하였다.\n"," \n","```\n","# 모두 동일한 확률로 초기화\n","self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                        for _ in range(env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["마지막 상태, 즉 에이전트가 도달하고자 하는 상태를 정의하기 위해 terminal state를 정의하였다.\n","\n","```\n","# terminal state 설정\n","self.policy_table[2][2] = []\n","```"]},{"cell_type":"markdown","metadata":{},"source":["벨만 방정식에 사용되는 할인율을 0.9로 정의해주었다.\n","\n","```\n","# 할인율 0.9\n","self.discount_factor = 0.9\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### policy_evaluation"]},{"cell_type":"markdown","metadata":{},"source":["```\n","# 벨만 기대 방정식으로 정책 평가\n","def policy_evaluation(self):\n","  # 다음 가치함수 초기화\n","  next_value_table = [[0.00] * self.env.width\n","                      for _ in range(self.env.height)]\n","\n","  # 모든 상태에 대해서 벨만 기대 방정식 계산\n","  for state in self.env.get_all_states():\n","    value = 0.0\n","\n","    if state != [2,2]: \n","      continue\n","    else: # terminal state\n","      next_value_table[state[0]][state[1]] = value\n","\n","    # 벨만 기대 방정식\n","    for action in self.env.possible_actions:\n","      next_state = self.env.state_after_action(state, action)\n","      reward = self.env.get_reward(state,action)\n","      next_value = self.get_value(next_state)\n","\n","      value += (self.get_policy(state)[action] *\n","                (reward + self.discount_factor * next_value))\n","      \n","    next_value_table[state[0]][state[1]] = value\n","\n","  self.value_table = next_value_table\n","```"]},{"cell_type":"markdown","metadata":{},"source":["정책 평가에서 에이전트는 모든 상태의 가치함수를 업데이트 한다. 이를 위해 `next_value_table`을 선언한 후 계산 결과를 저장한다. 이는 위에서 `value_table`을 선언하는 방식과 동일하게 진행된다.\n","\n","```\n","# 다음 가치함수 초기화\n","next_value_table = [[0.00] * self.env.width\n","                    for _ in range(self.env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["모든 상태에 대해 벨만 기대 방정식을 계산한다.\n","\n","```\n","# 모든 상태에 대해서 벨만 기대 방정식 계산\n","for state in self.env.get_all_states():\n","    value = 0.0\n","\n","    if state == [2,2]: # terminal state\n","        next_value_table[state[0]][state[1]] = value\n","        continue\n","\n","    # 벨만 기대 방정식\n","    for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state,action)\n","        next_value = self.get_value(next_state)\n","\n","        value += (self.get_policy(state)[action] *\n","                (reward + self.discount_factor * next_value))\n","        \n","    next_value_table[state[0]][state[1]] = value\n","```\n","\n","위의 벨만 기대 방정식을 수식으로 나타내면 다음과 같다.\n","\n","$v_{k+1}(s) = \\Sigma_{a\\in A}\\pi(a|s)(r_{(s,a)} + \\gamma v_k(s'))$\n","\n","$P^a_{ss'}$이 생략된 이유는 상태 변환 확률을 1로 설정했기 때문이다.\n","\n","action을 취했을 때 다음 상태가 어디인지 알려주는 역할은 `env.state_after_action(state, action)`이 수행한다."]},{"cell_type":"markdown","metadata":{},"source":["모든 상태에 대해 벨만 기대 방정식의 계산이 끝나면 현재의 `value_table`에 `next_value_table`을 덮어씌우는 방식으로 정책 평가를 진행한다.\n","\n","```\n","self.value_table = next_value_table\n","```"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOwFjSQmIjwKsWCtlSPLZg8","collapsed_sections":[],"name":"Policy_iteration_basic.ipynb","provenance":[]},"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
