{"cells":[{"cell_type":"markdown","metadata":{"id":"1Q2mN5F7h-bS"},"source":["# 정책 이터레이션\n"]},{"cell_type":"markdown","metadata":{"id":"Ig1B1628iLyx"},"source":["해당 노트북은 정책 이터레이션을 그리드월드에서 구현하는 예제 소스코드 `policy_iteration.py`와 `environment.py`에 대한 정리 노트북이다.\n","\n","1. `policy_iteration.py`\n","\n","  Policyiteration 클래스를 포함하며, 해당 클래스에는 정책 이터레이션 함수 및 main 함수가 포함되어 있다.\n","\n","2. `environment.py`\n","\n","  그리드월드 예제 화면을 구성하고, 상태, 보상 등 환경에 대한 정보를 제공하기 위한 함수로 구성되어 있다. 해당 코드는 깃허브의 'rlcode/reinforcement-learning-kr-v2'로부터 가져왔다. \n","  \n","  그리드월드의 경우 환경을 직접 만든 것이기 때문에 이러한 파일이 필요하나, 일반적으로는 이미 구축되어 있는 환경에 강화학습을 적용하는 경우가 많으므로 에이전트와 관련된 파일만 필요한 경우가 많다."]},{"cell_type":"markdown","metadata":{"id":"RvdFQKUJjlk8"},"source":["## 1. `policy_iteration.py`"]},{"cell_type":"markdown","metadata":{"id":"us2_TGoiwFeC"},"source":["main 함수를 제외한 `policy_iteration.py`는 다음과 같다."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":521,"status":"ok","timestamp":1652685913460,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"q7fTJ_vcopD2"},"outputs":[],"source":["import numpy as np\n","from environment import GraphicDisplay, Env\n","\n","class Policyiteration:\n","  def __init__(self, env):\n","    self.env = env\n","\n","    # 가치함수를 2차원 리스트로 초기화\n","    self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","\n","    # 모두 동일한 확률로 초기화\n","    self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                         for _ in range(env.height)]\n","\n","    # terminal state 설정\n","    self.policy_table[2][2] = []\n","\n","    # 할인율 0.9\n","    self.discount_factor = 0.9\n","  \n","  # 벨만 기대 방정식으로 정책 평가\n","  def policy_evaluation(self):\n","    # 다음 가치함수 초기화\n","    next_value_table = [[0.00] * self.env.width\n","                        for _ in range(self.env.height)]\n","\n","    # 모든 상태에 대해서 벨만 기대 방정식 계산\n","    for state in self.env.get_all_states():\n","      value = 0.0\n","\n","      if state != [2,2]: \n","        continue\n","      else: # terminal state\n","        next_value_table[state[0]][state[1]] = value\n","\n","      # 벨만 기대 방정식\n","      for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state,action)\n","        next_value = self.get_value(next_state)\n","\n","        value += (self.get_policy(state)[action] *\n","                  (reward + self.discount_factor * next_value))\n","        \n","      next_value_table[state[0]][state[1]] = value\n","\n","    self.value_table = next_value_table\n","\n","  # 현재 가치함수에 대해 탐욕 정책 발전\n","  def policy_improvement(self):\n","        next_policy = self.policy_table\n","        for state in self.env.get_all_states():\n","            if state == [2, 2]:\n","                continue\n","            \n","            value_list = []\n","\n","            # 정책 초기화\n","            result = [0.0, 0.0, 0.0, 0.0]\n","\n","            # 모든 행동에 대해 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n","            for index, action in enumerate(self.env.possible_actions):\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                value = reward + self.discount_factor * next_value\n","                value_list.append(value)\n","\n","            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","            max_idx_list = max_idx_list.flatten().tolist()\n","            prob = 1 / len(max_idx_list)\n","\n","            for idx in max_idx_list:\n","                result[idx] = prob\n","\n","            next_policy[state[0]][state[1]] = result\n","\n","        self.policy_table = next_policy\n","\n","  # 특정 상태에서 정책에 따라 무작위로 행동 반환\n","  def get_action(self, state):\n","      policy = self.get_policy(state)\n","      policy = np.array(policy)\n","      return np.random.choice(4, 1, p=policy)[0]\n","\n","  # 상태에 따른 정책 반환\n","  def get_policy(self, state):\n","      return self.policy_table[state[0]][state[1]]\n","\n","  # 가치함수 값을 반환tar -xzvf tk.tar.gz\n","  def get_value(self, state):\n","      return self.value_table[state[0]][state[1]]"]},{"cell_type":"markdown","metadata":{"id":"ZRtySmjEjxQn"},"source":["`policy_iteration.py`에서는 먼저 `environment.py`의 GraphicDisplay와 Env 클래스를 import 한다."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1652683253858,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"xLfKV5r8hrkb"},"outputs":[],"source":["from environment import GraphicDisplay, Env"]},{"cell_type":"markdown","metadata":{"id":"NvmZt9zBn3qF"},"source":["정책 이터레이션의 에이전트는 `policy_iteration.py` 파일 내에서 Policyiteration 클래스로 정의되어 있다. Env 클래스로 정의한 객체 env를 Policyiteration 클래스의 인수로 전달함으로써 에이전트가 환경의 Env 클래스에 접근할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["다음 함수를 실행하면 그리드월드 화면이 나오게 된다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":290,"status":"error","timestamp":1652685853201,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"j-n7pWJamSQd","outputId":"845ba865-6add-4817-c266-f02d31084795"},"outputs":[],"source":["# main 함수\n","# if __name__ == '__main__':\n","#   env = Env()\n","#   policy_iteration = Policyiteration(env)\n","#   grid_world = GraphicDisplay(policy_iteration)\n","#   grid_world.mainloop()"]},{"cell_type":"markdown","metadata":{},"source":["**이제 `policy_iteration.py`의 코드를 하나씩 분석해보도록 하자.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbSIoNAywM4i"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOwFjSQmIjwKsWCtlSPLZg8","collapsed_sections":[],"name":"Policy_iteration_basic.ipynb","provenance":[]},"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
