{"cells":[{"cell_type":"markdown","metadata":{"id":"1Q2mN5F7h-bS"},"source":["# 정책 이터레이션\n"]},{"cell_type":"markdown","metadata":{"id":"Ig1B1628iLyx"},"source":["해당 노트북은 정책 이터레이션을 그리드월드에서 구현하는 예제 소스코드 `policy_iteration.py`와 `environment.py`에 대한 정리 노트북이다.\n","\n","1. `policy_iteration.py`\n","\n","  Policyiteration 클래스를 포함하며, 해당 클래스에는 정책 이터레이션 함수 및 main 함수가 포함되어 있다.\n","\n","2. `environment.py`\n","\n","  그리드월드 예제 화면을 구성하고, 상태, 보상 등 환경에 대한 정보를 제공하기 위한 함수로 구성되어 있다. 해당 코드는 깃허브의 'rlcode/reinforcement-learning-kr-v2'로부터 가져왔다. \n","  \n","  그리드월드의 경우 환경을 직접 만든 것이기 때문에 이러한 파일이 필요하나, 일반적으로는 이미 구축되어 있는 환경에 강화학습을 적용하는 경우가 많으므로 에이전트와 관련된 파일만 필요한 경우가 많다."]},{"cell_type":"markdown","metadata":{"id":"RvdFQKUJjlk8"},"source":["## 1. `policy_iteration.py`"]},{"cell_type":"markdown","metadata":{"id":"us2_TGoiwFeC"},"source":["main 함수를 제외한 `policy_iteration.py`는 다음과 같다."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":521,"status":"ok","timestamp":1652685913460,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"q7fTJ_vcopD2"},"outputs":[],"source":["import numpy as np\n","from environment import GraphicDisplay, Env\n","\n","class Policyiteration:\n","  def __init__(self, env):\n","    self.env = env\n","\n","    # 가치함수를 2차원 리스트로 초기화\n","    self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","\n","    # 모두 동일한 확률로 초기화\n","    self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                         for _ in range(env.height)]\n","\n","    # terminal state 설정\n","    self.policy_table[2][2] = []\n","\n","    # 할인율 0.9\n","    self.discount_factor = 0.9\n","  \n","  # 벨만 기대 방정식으로 정책 평가\n","  def policy_evaluation(self):\n","    # 다음 가치함수 초기화\n","    next_value_table = [[0.00] * self.env.width\n","                        for _ in range(self.env.height)]\n","\n","    # 모든 상태에 대해서 벨만 기대 방정식 계산\n","    for state in self.env.get_all_states():\n","      value = 0.0\n","\n","      if state == [2,2]: \n","        next_value_table[state[0]][state[1]] = value\n","        continue\n","\n","      # 벨만 기대 방정식\n","      for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state,action)\n","        next_value = self.get_value(next_state)\n","\n","        value += (self.get_policy(state)[action] *\n","                  (reward + self.discount_factor * next_value))\n","        \n","      next_value_table[state[0]][state[1]] = value\n","\n","    self.value_table = next_value_table\n","\n","  # 현재 가치함수에 대해 탐욕 정책 발전\n","  def policy_improvement(self):\n","        next_policy = self.policy_table\n","        for state in self.env.get_all_states():\n","            if state == [2, 2]:\n","                continue\n","            \n","            value_list = []\n","\n","            # 정책 초기화\n","            result = [0.0, 0.0, 0.0, 0.0]\n","\n","            # 모든 행동에 대해 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n","            for index, action in enumerate(self.env.possible_actions):\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                value = reward + self.discount_factor * next_value\n","                value_list.append(value)\n","\n","            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","            max_idx_list = max_idx_list.flatten().tolist()\n","            prob = 1 / len(max_idx_list)\n","\n","            for idx in max_idx_list:\n","                result[idx] = prob\n","\n","            next_policy[state[0]][state[1]] = result\n","\n","        self.policy_table = next_policy\n","\n","  # 특정 상태에서 정책에 따라 무작위로 행동 반환\n","  def get_action(self, state):\n","      policy = self.get_policy(state)\n","      policy = np.array(policy)\n","      return np.random.choice(4, 1, p=policy)[0]\n","\n","  # 상태에 따른 정책 반환\n","  def get_policy(self, state):\n","      return self.policy_table[state[0]][state[1]]\n","\n","  # 가치함수 값을 반환\n","  def get_value(self, state):\n","      return self.value_table[state[0]][state[1]]"]},{"cell_type":"markdown","metadata":{"id":"ZRtySmjEjxQn"},"source":["`policy_iteration.py`에서는 먼저 `environment.py`의 GraphicDisplay와 Env 클래스를 import 한다."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1652683253858,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"xLfKV5r8hrkb"},"outputs":[],"source":["from environment import GraphicDisplay, Env"]},{"cell_type":"markdown","metadata":{"id":"NvmZt9zBn3qF"},"source":["정책 이터레이션의 에이전트는 `policy_iteration.py` 파일 내에서 Policyiteration 클래스로 정의되어 있다. Env 클래스로 정의한 객체 env를 Policyiteration 클래스의 인수로 전달함으로써 에이전트가 환경의 Env 클래스에 접근할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["다음 함수를 실행하면 그리드월드 화면이 나오게 된다."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":290,"status":"error","timestamp":1652685853201,"user":{"displayName":"Kevin Rain","userId":"16288063605288038068"},"user_tz":-540},"id":"j-n7pWJamSQd","outputId":"845ba865-6add-4817-c266-f02d31084795"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]}],"source":["# main 함수\n","if __name__ == '__main__':\n","  env = Env()\n","  policy_iteration = Policyiteration(env)\n","  grid_world = GraphicDisplay(policy_iteration)\n","  grid_world.mainloop()"]},{"cell_type":"markdown","metadata":{},"source":["출력되는 화면에서 Evaluation을 누르면 `policy_evaluation()` 함수가, Improvement 버튼을 누르면 `policy_improvement()` 함수가 실행된다. 즉, 두 버튼을 번갈아가면서 누르면 최적 정책에 수렴할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["그렇게 정책이 최적 정책에 수렴이 되어 더 이상의 발전이 이루어지지 않을 때 (그리드월드는 간단한 환경이므로 수렴 이후 정책 변동이 거의 없다) Move 버튼을 누르면 `get_action(state)` 함수가 실행되며 에이전트가 정책에 따라 행동을 수행한다.\n","\n","Reset 버튼을 누르면 모든 변수를 초기화시켜 다시 진행할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["**이제 `policy_iteration.py`의 코드를 하나씩 분석해보도록 하자.**"]},{"cell_type":"markdown","metadata":{},"source":["### \\_\\_init\\_\\_"]},{"cell_type":"markdown","metadata":{},"source":["\n","    def __init__(self, env):\n","        self.env = env\n","\n","        # 가치함수를 2차원 리스트로 초기화\n","        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","\n","        # 모두 동일한 확률로 초기화\n","        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                                for _ in range(env.height)]\n","\n","        # terminal state 설정\n","        self.policy_table[2][2] = []\n","\n","        # 할인율 0.9\n","        self.discount_factor = 0.9"]},{"cell_type":"markdown","metadata":{},"source":["먼저 Policyiteration 코드 내에서 가장 먼저 정의되는 \\_\\_init\\_\\_ 함수를 살펴보자.\n","\n","해당 함수는 먼저 env를 self.env로 정의함으로써 환경에 대한 객체를 선언한다.\n","\n","env 객체에 정의되어 있는 변수 및 함수는 다음과 같다."]},{"cell_type":"markdown","metadata":{},"source":["| 코드 | 설명 | 반환값|\n","|-----|-----|-----|\n","|`env.width`, `env.height`|그리드월드의 너비와 높이|그리드월드의 가로, 세로를 정수로 반환|\n","|`env.state_after_action(state, action)`|state에서 action을 했을 때 에이전트가 가는 다음 상태|행동 후의 상태 좌표를 리스트로 반환|\n","|`env.get_all_states()`|존재하는 모든 상태|모든 상태를 반환|\n","|`env.get_reward(state, action)`|state의 보상|보상을 정수 형태로 반환|\n","|`env.possible_actions`|상, 하, 좌, 우|[0,1,2,3] 반환. 순서대로 상하좌우|"]},{"cell_type":"markdown","metadata":{},"source":["정책 이터레이션은 모든 상태에 대해 가치함수를 계산하기 때문에 `value_table`이라는 2차원 리스트 변수로 가치함수를 선언한다.\n","\n","여기서 2차원 리스트의 크기는 `env.width`와 `env.height`로 정해지므로 그리드월드의 크기인 5*5이다.\n","\n","모든 상태의 가치함수에 대한 초기화 값은 0이다.\n","\n","\n","```\n","# 가치함수를 2차원 리스트로 초기화\n","self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["정책 `policy_table`은 모든 상태에 대해 상, 하, 좌, 우에 해당하는 각 행동의 확률을 담고 있는 리스트이다.\n","\n","따라서 5 * 5 * 4의 크기를 가지는 3차원 리스트이며, 모든 행동에 대하여 같은 확률인 0.25로 초기화를 진행하였다.\n"," \n","```\n","# 모두 동일한 확률로 초기화\n","self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                        for _ in range(env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["마지막 상태, 즉 에이전트가 도달하고자 하는 상태를 정의하기 위해 terminal state를 정의하였다.\n","\n","```\n","# terminal state 설정\n","self.policy_table[2][2] = []\n","```"]},{"cell_type":"markdown","metadata":{},"source":["벨만 방정식에 사용되는 할인율을 0.9로 정의해주었다.\n","\n","```\n","# 할인율 0.9\n","self.discount_factor = 0.9\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### policy_evaluation"]},{"cell_type":"markdown","metadata":{},"source":["```\n","# 벨만 기대 방정식으로 정책 평가\n","def policy_evaluation(self):\n","  # 다음 가치함수 초기화\n","  next_value_table = [[0.00] * self.env.width\n","                      for _ in range(self.env.height)]\n","\n","  # 모든 상태에 대해서 벨만 기대 방정식 계산\n","  for state in self.env.get_all_states():\n","    value = 0.0\n","\n","    if state != [2,2]: \n","      continue\n","    else: # terminal state\n","      next_value_table[state[0]][state[1]] = value\n","\n","    # 벨만 기대 방정식\n","    for action in self.env.possible_actions:\n","      next_state = self.env.state_after_action(state, action)\n","      reward = self.env.get_reward(state,action)\n","      next_value = self.get_value(next_state)\n","\n","      value += (self.get_policy(state)[action] *\n","                (reward + self.discount_factor * next_value))\n","      \n","    next_value_table[state[0]][state[1]] = value\n","\n","  self.value_table = next_value_table\n","```"]},{"cell_type":"markdown","metadata":{},"source":["정책 평가에서 에이전트는 모든 상태의 가치함수를 업데이트 한다. 이를 위해 `next_value_table`을 선언한 후 계산 결과를 저장한다. 이는 위에서 `value_table`을 선언하는 방식과 동일하게 진행된다.\n","\n","```\n","# 다음 가치함수 초기화\n","next_value_table = [[0.00] * self.env.width\n","                    for _ in range(self.env.height)]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["모든 상태에 대해 벨만 기대 방정식을 계산한다.\n","\n","```\n","# 모든 상태에 대해서 벨만 기대 방정식 계산\n","for state in self.env.get_all_states():\n","    value = 0.0\n","\n","    if state == [2,2]: # terminal state\n","        next_value_table[state[0]][state[1]] = value\n","        continue\n","\n","    # 벨만 기대 방정식\n","    for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state,action)\n","        next_value = self.get_value(next_state)\n","\n","        value += (self.get_policy(state)[action] *\n","                (reward + self.discount_factor * next_value))\n","        \n","    next_value_table[state[0]][state[1]] = value\n","```\n","\n","위의 벨만 기대 방정식을 수식으로 나타내면 다음과 같다.\n","\n","$v_{k+1}(s) = \\Sigma_{a\\in A}\\pi(a|s)(r_{(s,a)} + \\gamma v_k(s'))$\n","\n","$P^a_{ss'}$이 생략된 이유는 상태 변환 확률을 1로 설정했기 때문이다.\n","\n","action을 취했을 때 다음 상태가 어디인지 알려주는 역할은 `env.state_after_action(state, action)`이 수행한다."]},{"cell_type":"markdown","metadata":{},"source":["모든 상태에 대해 벨만 기대 방정식의 계산이 끝나면 현재의 `value_table`에 `next_value_table`을 덮어씌우는 방식으로 정책 평가를 진행한다.\n","\n","```\n","self.value_table = next_value_table\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### policy_improvement"]},{"cell_type":"markdown","metadata":{},"source":["```\n","# 현재 가치함수에 대해 탐욕 정책 발전\n","def policy_improvement(self):\n","    next_policy = self.policy_table\n","    for state in self.env.get_all_states():\n","        if state == [2, 2]:\n","            continue\n","        \n","        value_list = []\n","\n","        # 정책 초기화\n","        result = [0.0, 0.0, 0.0, 0.0]\n","\n","        # 모든 행동에 대해 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n","        for index, action in enumerate(self.env.possible_actions):\n","            next_state = self.env.state_after_action(state, action)\n","            reward = self.env.get_reward(state, action)\n","            next_value = self.get_value(next_state)\n","            value = reward + self.discount_factor * next_value\n","            value_list.append(value)\n","\n","        # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","        max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","        max_idx_list = max_idx_list.flatten().tolist()\n","        prob = 1 / len(max_idx_list)\n","\n","        for idx in max_idx_list:\n","            result[idx] = prob\n","\n","        next_policy[state[0]][state[1]] = result\n","\n","    self.policy_table = next_policy\n","```"]},{"cell_type":"markdown","metadata":{},"source":["정책 평가를 통해 정책을 평가하면 그에 따른 새로운 가치함수를 얻는다.\n","\n","에이전트는 이렇게 얻어진 새로운 가치함수를 이용해 정책을 업데이트한다."]},{"cell_type":"markdown","metadata":{},"source":["정책 평가에서와 비슷하게, 정책 `policy_table`을 복사한 `next_policy`에 업데이트 된 정책을 저장한다.\n","\n","```\n","next_policy = self.policy_table\n","```\n","\n","이때 정책의 업데이트에는 탐욕 정책 발전 방법을 사용한다.\n","\n","```\n","# 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","max_idx_list = max_idx_list.flatten().tolist()\n","prob = 1 / len(max_idx_list)\n","```"]},{"cell_type":"markdown","metadata":{},"source":["탐욕 정책 발전은 가치가 가장 높은 하나의 행동을 선택하는 것인데, 이것은 여러 개일 수 있다. 이 경우 각 최적 행동을 다시 동일한 확률로 선택하는 정책으로 업데이트한다."]},{"cell_type":"markdown","metadata":{},"source":["탐욕 정책을 구하는 순서는 다음과 같다.\n","\n","1. 현재 상태에서 가능한 행동에 대해 $r_{(s,a)} + \\gamma v_k(s')$을 계산한다. 이 결과는 `value_list`에 저장한다.\n","    ```\n","    for index, action in enumerate(self.env.possible_actions):\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state, action)\n","        next_value = self.get_value(next_state)\n","        value = reward + self.discount_factor * next_value\n","        value_list.append(value)\n","    ```\n","\n","2. `value_list`에 담긴 값 중 가장 큰 값을 max 함수를 통해 알아낸다. 그 후 numpy의 argwhere 함수를 통해 가장 큰 값의 index를 알아낸다.\n","    ```\n","    max_idx_list = np.argwhere(value_list == np.amax(value_list))\n","    ```\n","\n","3. `max_idx_list`에 담긴 값이 여러 개라면(최적 행동이 여러 개라면) 에이전트는 해당 리스트 내 index들을 동일한 확률로 선택한다. 이를 구현하기 위해 1을 `max_idx_list`의 길이로 나누어 행동의 확률을 계산한다. 이후 해당 행동에 계산한 확률값을 저장한다.\n","\n","    ```\n","    max_idx_list = max_idx_list.flatten().tolist()\n","    prob = 1 / len(max_idx_list)\n","\n","    for idx in max_idx_list:\n","        result[idx] = prob\n","    ```"]},{"cell_type":"markdown","metadata":{},"source":["### get_action"]},{"cell_type":"markdown","metadata":{},"source":["```  \n","# 특정 상태에서 정책에 따라 무작위로 행동 반환\n","def get_action(self, state):\n","    policy = self.get_policy(state)\n","    policy = np.array(policy)\n","    return np.random.choice(4, 1, p=policy)[0]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["에이전트가 정책에 따라 움직이려면 특정 상태에서 어떤 행동을 해야할지 알아야 하는데, 그 역할을 하는 것이 `get_action` 함수이다."]},{"cell_type":"markdown","metadata":{},"source":["정책은 각 행동을 할 확률이므로 확률에 기반하여 행동을 선택해야하고, 이를 수행하는 것이 `np.random.choice`이다.\n","\n","첫 번째 인자는 행동의 개수이고 두 번째 인자는 몇 개의 행동을 샘플링할지 정하며, 세 번째 인자는 각 행동을 샘플링할 확률로, policy이다.\n","\n","```\n","return np.random.choice(4, 1, p=policy)[0]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### get_policy, get_value"]},{"cell_type":"markdown","metadata":{},"source":["```\n","# 상태에 따른 정책 반환\n","def get_policy(self, state):\n","    return self.policy_table[state[0]][state[1]]\n","\n","# 가치함수 값을 반환\n","def get_value(self, state):\n","    return self.value_table[state[0]][state[1]]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["`get_policy`의 경우 `self.policy_table`로 저장되어 있는 정책에서 해당 상태에 대한 정책을 반환한다.\n","\n","`get_value`의 경우 `self.value_table`로 저장되어 있는 가치함수에서 해당 상태에 해당하는 가치함수를 반환한다."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOwFjSQmIjwKsWCtlSPLZg8","collapsed_sections":[],"name":"Policy_iteration_basic.ipynb","provenance":[]},"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
